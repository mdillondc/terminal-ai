# Search‑Log: Local Conversation Search

Search‑Log lets you search the current conversation log on disk using AI‑generated keywords plus a mature local text‑search engine. It returns precise, sentence‑level snippets with timestamps, appends them to the conversation as a single system message, and the AI answers your question based on that appended summary. No persistent index. No rebuild/status commands. Always fresh by reading the log file at search time.

---

## Why this exists

- Fast, precise retrieval from long conversations without overloading the AI’s context window
- Timeline questions (“when did we…”) and factual lookups (“find where we mentioned X”) need reliable timestamps and concise snippets
- Keep it simple: avoid index lifecycle complexity while still using proven search behavior

---

## What it does

- Uses the active LLM to generate search keywords from your prompt (no exposed temperature/token knobs)
- Builds an in‑memory Whoosh index per search (ephemeral; no disk index)
- Searches per‑message documents (not the entire log as one blob)
- Extracts sentence‑aware snippets around matches, with timestamps and message IDs
- Detects temporal queries and groups results by date
- Appends one system message containing the search results via the standard conversation append path
- The AI then answers based only on this compact result summary

---

## High‑level flow

1) User asks a question while `--search-log` is enabled  
2) Keyword extraction (LLM)  
3) Fresh read of the active JSON conversation file from disk  
4) Build an in‑memory Whoosh index (one document per message)  
5) Run a keyword query (OR semantics, phrase queries boosted)  
6) Take top K anchor hits (K = `search_log_max_results`)  
7) For each hit, generate N sentence‑aware snippets (configurable; fallback to line or bounded context if needed)  
8) If temporal, group by date; otherwise keep relevance order  
9) Append one system message with snippets using `conversation_manager` (not direct file writes)  
10) AI answers the user based on the appended results

Always fresh: We re‑read the on‑disk JSON per search, so results reflect the latest appended messages.

---

## Data model and indexing

- Source of truth: the active conversation JSON file on disk (array of messages)
- Each message becomes one Whoosh document, with stored metadata:
  - `content` (TEXT, stored)
  - `timestamp` (NUMERIC, stored)
  - `role` (ID, stored)
  - `turn_index` (NUMERIC, stored)
  - `message_id` (NUMERIC, stored)

Because fields are stored, every match/snippet maps unambiguously to its message and timestamp. Indexing includes only user and assistant roles; system messages are excluded to avoid searching search results and raw web-search dumps.

---

## Query semantics

- Keywords: generated by the active LLM from the user’s question and recent context
- Query: OR across keywords for recall
- Phrase weighting: multi‑word keywords also form a phrase query variant to boost exact phrase matches above loose token hits
- Analyzer: Whoosh StandardAnalyzer (stoplist=None) for multilingual support (no stopwords)
- Temporal intent: the LLM classifies whether the user’s query is temporal; if yes, we group by date (YYYY‑MM‑DD). Fallback (when LLM is unavailable) uses only numeric date/time patterns (e.g., YYYY‑MM‑DD, DD/MM/YYYY, DD.MM.YYYY, HH:MM).

---

## Snippet generation (sentence‑aware)

- Primary: Sentence‑aware fragmenter to return whole sentences around the match
- Count: N fragments per hit (configurable via `search_log_snippets_per_hit`, default 1). Prioritize clarity and relevance.
- Fallback: If text lacks clear sentence boundaries (e.g., code blocks), fall back to a fixed‑width context fragmenter
- Optional neighboring messages: You can include ±N neighboring messages (controlled by `search_log_context_messages`, recommend 0 by default). We deduplicate to avoid duplicates when hits cluster.

Result quality is prioritized over tight size constraints.

---

## Result packaging and appending

- We build a single, compact system message that includes:
  - The keywords used
  - A note on whether the query was treated as temporal
  - For each hit: timestamp, message_id, role, turn_index, and N sentence‑level snippets (per settings)
  - If temporal: grouped by date; within each date, entries sorted by time
  - If non‑temporal: entries sorted by relevance score

- This message is appended using the same API the app uses to append any message (via `conversation_manager`). Its content is prefixed with 'Search-Log Summary:' for easy identification. We do not write the JSON file directly. If no results are available (no query terms, no active log/indexable messages, or zero matches), no system message is appended and the AI response is skipped; control returns to the prompt.

- The AI’s next answer uses only this appended system message as the basis for its response (plus normal prompt context), keeping context usage efficient and precise.

---

## Settings

Keep (minimal and practical):
- `search_log` (bool): toggle the feature on/off
- `search_log_transparency` (bool): show keywords used, hit counts, and whether temporal grouping was applied
- `search_log_context_messages` (int): number of neighboring messages per side to include for each hit (recommended default: 0). Example: 2 means up to 2 before and 2 after (user/assistant only, deduped).
- `search_log_snippets_per_hit` (int): number of sentence-bounded snippets to include per matched message (default: 1). Increase to 2 if you want slightly more local context.
- `search_log_max_results` (int): anchor hits cap before adding snippets and optional neighbors (default: 400)

Commands:
- Keep: `--search-log` (toggle)

---

## Implementation design (components)

- KeywordExtractor
  - Input: user query + recent conversation context
  - Output: a set of keywords and phrases
  - Classifies query as temporal or not

- LogReader
  - Fetches the active conversation file path from the conversation manager
  - Reads and parses the JSON array from disk

- SearchEngine (Whoosh, per‑search in‑memory)
  - Builds a RAM index from current messages (one doc per message with stored metadata)
  - Executes the query (OR, with phrase boosts)
  - Returns top K hits and scores

- SnippetRenderer
  - For each hit: generates N sentence‑aware fragments (configurable; fallback to line or bounded context if needed)
  - Optional: includes ±N neighboring messages based on `search_log_context_messages`
  - Deduplicates entries

- ResultGrouper
  - Temporal: groups snippets by date (YYYY‑MM‑DD) and orders chronologically
  - Non‑temporal: sorts by relevance

- ConversationAppender
  - Creates one system message containing the result summary
  - Appends it via the standard conversation append path (so the AI will see it)

- Transparency
  - When `search_log_transparency` is true, prints: keywords, hit counts, temporal grouping yes/no

No persistent index. No rebuild/status flows. Everything is scoped to the single active conversation.

---

## Usage examples (sanitized)

Example 1: Timeline query
```
> --search-log
search-log enabled

> On which dates did we discuss deployment steps?
• Keywords: deployment, steps, rollout, release
• Treating as temporal: yes
• Searching conversation log...
• 19 matches across 4 dates

Timeline:
• 2025-01-12
  - 09:44 (msg 102): “… initial deployment steps for staging …”
• 2025-01-13
  - 10:03 (msg 126): “… verify rollout plan and rollback strategy …”
• 2025-01-15
  - 14:22 (msg 178): “… release checklist finalized …”
• 2025-01-20
  - 16:07 (msg 241): “… production deployment window confirmed …”

• Appended search summary to conversation (system)
```

Example 2: Fact finding
```
> Which file path did we decide to use for configs?
• Keywords: config, configuration, path, directory, file
• Treating as temporal: no
• Searching conversation log...
• 7 matches

Results (relevance):
1) 2025-02-04 11:18 (msg 87): “… the configuration path will be /etc/myapp/config.yml …”
2) 2025-02-05 09:51 (msg 122): “… confirm that /etc/myapp/config.yml is readable …”
3) 2025-02-05 10:10 (msg 129): “… update the config file to include the new section …”

• Appended search summary to conversation (system)
```

Example 3: Topic exploration
```
> Show me all mentions of logging improvements
• Keywords: logging, logs, instrumentation, telemetry
• Treating as temporal: no
• Searching conversation log...
• 12 matches

Results (relevance):
- 2025-03-01 08:30 (msg 34): “… add structured logging for request traces …”
- 2025-03-02 13:42 (msg 77): “… increase log level around retries …”
- 2025-03-03 15:09 (msg 119): “… telemetry pipeline improvements …”

• Appended search summary to conversation (system)
```

---

## Defaults and behavior notes

- Default analyzer: Whoosh StandardAnalyzer (stoplist=None) for multilingual support (no stopwords)
- Phrase weighting: enabled (phrase variants of multi‑word keywords are boosted)
- Snippets per hit: configurable via `search_log_snippets_per_hit` (default 1). Sentence‑aware fragments (fallback to line‑based or bounded context when punctuation is absent)
- `search_log_context_messages`: recommended default 0 (snippets typically suffice); can be set to 1 if you want extra neighboring context
- `search_log_max_results`: 400 (anchor hits; snippets and optional neighbors follow)
- Freshness: the log file is re‑read on each search; no stale results
- Appending: always via `conversation_manager`’s standard append path (prefixed with 'Search-Log Summary:') (never write JSON files directly). If no results (no query terms, no active log/indexable messages, or zero matches), skip the AI response and return to the prompt.

---

## Success criteria

- Accurate, sentence‑level snippets with correct timestamps and message IDs
- Timeline grouping when the query is temporal
- Minimal complexity: no persistent index, rebuilds, or status commands
- The AI produces better answers based on the appended search summary than it would from raw memory alone